--- 
title: "Dialect Identification (dialectid)"
format: 
  dashboard:
    logo: images/ingeotec.png
    orientation: columns
    nav-buttons: [github]
    theme: cosmo
execute:
  freeze: auto    
---

```{python} 
#| echo: false 
#| include: false
#| label: setup

from dialectid.utils import BASEURL, COUNTRIES
from dialectid import DialectId
from IPython.display import Markdown
``` 

# Introduction

## Column 

::: {.card title='Introduction' .flow}  
`dialectid` aims to develop a set of algorithms to detect the dialect of a given text. For example, given a text written in Spanish, `dialectid` predicts the Spanish-speaking country where the text comes from. 

`dialectid` is available for Arabic (ar), German (de), English (en), Spanish (es), French (fr), Dutch (nl), Portuguese (pt), Russian (ru), Turkish (tr), and Chinese (zh).
:::

::: {.card title='Installing using conda' .flow}

`dialectid` can be install using the conda package manager with the following instruction.

```{sh} 
conda install --channel conda-forge dialectid
``` 
::: 

::: {.card title='Installing using pip' .flow} 
A more general approach to installing `dialectid` is through the use of the command pip, as illustrated in the following instruction.

```{sh} 
pip install dialectid
```
::: 

::: {.card title='Countries' .flow}
```{python}
#| echo: true
#| label: countries

from dialectid import DialectId
detect = DialectId(lang='es')
detect.countries
```
:::

## Column 

::: {.card title='Dialect Identification' .flow}
```{python} 
#| echo: true
#| label: Identification

from dialectid import DialectId
detect = DialectId(lang='es')
detect.predict(['comiendo unos tacos',
                'acompañando el asado con un buen vino'])
```
:::

::: {.card title='Decision Function' .flow}
```{python} 
#| echo: true
#| label: Distance

from dialectid import DialectId
detect = DialectId(lang='es')
df = detect.decision_function(['acompañando el asado con un buen vino'])[0]
index = df.argsort()[::-1]
[(detect.countries[i], df[i]) for i in index
 if df[i] > 0]
```
:::

::: {.card title='Probability' .flow}
```{python} 
#| echo: true
#| label: Probability

from dialectid import DialectId
detect = DialectId(lang='es', probability=True)
prob = detect.predict_proba(['acompañando el asado con un buen vino'])[0]
index = prob.argsort()[::-1]
[(detect.countries[i], prob[i])
 for i in index[:4]]
```
:::

# Corpus 

## Column {.tabset} 

```{python}
#| echo: false
#| label: Corpus

from collections import defaultdict
from IPython.display import Markdown
import pandas as pd
from encexp.download import download
from encexp.utils import DialectID_URL

def dataset_info(lang='es'):
    dataset = download('dialectid_dataset_info',
                       base_url=DialectID_URL)
    # data = []
    # for line in dataset:
    #     if line['lang'] != lang:
    #         continue
    #     data = defaultdict(dict)

    #     cntr = data[]
    dataset = {data['set']: {k: v for k, v in data.items() 
                             if k not in ('lang', 'set')}
               for data in dataset if data['lang'] == lang}
    dataset = pd.DataFrame(dataset).reset_index(names='Country')
    return dataset.sort_values('Country')
```


::: {.card title='Arabic (ar)'}
```{python}
#| echo: false
#| label: Arabic

Markdown(dataset_info('ar').to_markdown(index=False))
```
:::

::: {.card title='German (de)'}
```{python}
#| echo: false
#| label: German

Markdown(dataset_info('de').to_markdown(index=False))
```
:::

::: {.card title='English (en)'}
```{python}
#| echo: false
#| label: English

Markdown(dataset_info('en').to_markdown(index=False))
```
:::

::: {.card title='Spanish (es)'}
```{python}
#| echo: false
#| label: Spanish

Markdown(dataset_info('es').to_markdown(index=False))
```
:::

::: {.card title='French (fr)'}
```{python}
#| echo: false
#| label: French

Markdown(dataset_info('fr').to_markdown(index=False))
```
:::

::: {.card title='Dutch (nl)'}
```{python}
#| echo: false
#| label: Dutch

Markdown(dataset_info('nl').to_markdown(index=False))
```
:::

::: {.card title='Portuguese (pt)'}
```{python}
#| echo: false
#| label: Portuguese

Markdown(dataset_info('pt').to_markdown(index=False))
```
:::

::: {.card title='Russian (ru)'}
```{python}
#| echo: false
#| label: Russian

Markdown(dataset_info('ru').to_markdown(index=False))
```
:::

::: {.card title='Turkish (tr)'}
```{python}
#| echo: false
#| label: Turkish

Markdown(dataset_info('tr').to_markdown(index=False))
```
:::

::: {.card title='Chinese (zh)'}
```{python}
#| echo: false
#| label: Chinese

Markdown(dataset_info('zh').to_markdown(index=False))
```
:::

## Column 

::: {.card title="Description"}
The dataset used to create the self-supervised problems is a collection of Tweets collected from the open stream for several years, i.e., the Spanish collection started on December 11, 2015; English on July 1, 2016; Arabic on January 25, 2017; Russian on October 16, 2018; and the rest of the languages on June 1, 2021. In all the cases, the last day collected was June 9, 2023. The collected Tweets were filtered with the following restrictions: the retweets were removed; URL and users were replaced by the tokens _url and _usr, respectively; and only tweets with at least 50 characters were accepted in the final collection. 

The Corpus is divided into two sets: the first set is used as a training set, i.e., to estimate the parameters, while the second set corresponds to the test set, which could be used to measure the model performance. The basis for this division is a specific date, with tweets published prior to October 1, 2022, forming the first set and those published on October 3, 2022, or later, being used to create the test set. 

The training set and test set were created with an equivalent procedure; the only difference is that the maximum size of the training set is $2^{21}$ (2M) tweets and $2^{12}$ (4096) tweets for the test set.

The training and test set was meticulously crafted by uniformly selecting the maximum number of tweets (i.e., $2^{21}$ and $2^{12}$, respectively) from each day collected. These selected tweets were then organized by day, and within each day, the tweets were randomly chosen, with near duplicates being removed. The subsequent step involved the elimination of tweets that were near duplicates of the previous three days.

It is worth mentioning that the last step is to shuffle the training and test set to eliminate the ordering by date.
:::

# Performance

## Column {.tabset}

```{python}
#| echo: false

from dialectid import DialectId
from microtc.utils import tweet_iterator
from microtc.utils import save_model, load_model
from os.path import join, basename, isfile
from CompStats import metrics
from CompStats import measurements
import numpy as np
import pandas as pd
from glob import glob
import seaborn as sns
sns.set_style('whitegrid')
sns.set_theme(font_scale=1.2)


def performance(lang, score, prefix='',
                value_name='macro-recall', **kwargs):
    fname = f'perf/{prefix}{lang}.gz'
    if isfile(fname):
        return load_model(fname)
    pred_dirname = f'dialectid-datasets/predictions/{lang}'
    for fname_pred in glob(join(pred_dirname, '*.json')):
        data = next(tweet_iterator(fname_pred))
        key = basename(fname_pred).split('.json')[0]
        if key == 'y':
            continue
        score(np.array(data), name=key)

    _ = score.dataframe(comparison=True,
                        value_name=value_name, **kwargs)
    _['Language'] = lang
    save_model(_, fname)
    return _
```

::: {.card title='Macro-recall'}
```{python}
#| echo: false
#| label: macro-recall

df = pd.DataFrame()
for lang in ['es', 'en', 'ar',
             'de', 'fr', 'nl', 
             'pt', 'ru', 'tr',
             'zh']:
    pred_dirname = f'dialectid-datasets/predictions/{lang}'
    gold = next(tweet_iterator(join(pred_dirname, 'y.json')))
    if 'score' in gold:
        score = gold['score']
    else:
        score = 'macro_recall'
    score = getattr(metrics, score)(np.array(gold['y']))
    _ = performance(lang, score)
    df = pd.concat((df, _))
ci = lambda x: measurements.CI(x, alpha=0.05)
f_grid = sns.catplot(df, x='macro-recall', y='Algorithm', col_wrap=3,
                    # order=[code.get(x, x) for x in algs],
                    capsize=0.2, linestyle='none', col='Language',
                    kind='point', errorbar=ci, sharex=False,
                    hue='Comparison')
```
:::

::: {.card title='Arabic (recall)'}

```{python}
#| echo: false

def country_recall(lang, col_wrap=5):
    detect = DialectId(lang=lang)
    pred_dirname = f'dialectid-datasets/predictions/{lang}'
    gold = next(tweet_iterator(join(pred_dirname, 'y.json')))
    score = metrics.recall_score(np.array(gold['y']), average=None)

    df = performance(lang, score, prefix='recall_', value_name='Recall',
                     var_name='Country',
                     perf_names=[f'{cntr}' for cntr in detect.countries])
    df.drop(columns=['Language'], inplace=True)
    ci = lambda x: measurements.CI(x, alpha=0.05)
    f_grid = sns.catplot(df, x='Recall', y='Algorithm', col_wrap=col_wrap,
                        # order=[code.get(x, x) for x in algs],
                        capsize=0.2, linestyle='none', col='Country',
                        kind='point', errorbar=ci, # sharex=False,
                        hue='Comparison')
    return f_grid    
```

```{python}
#| echo: false
#| label: Arabic-perf

country_recall('ar')
```
::: 

::: {.card title='German (recall)'}
```{python}
#| echo: false
#| label: German-perf

country_recall('de', col_wrap=None)
```
::: 

::: {.card title='English (recall)'}
```{python}
#| echo: false
#| label: English-perf

country_recall('en', col_wrap=6)
```
::: 

::: {.card title='Spanish (recall)'}
```{python}
#| echo: false
#| label: Spanish-perf

country_recall('es')
```
::: 

::: {.card title='French (recall)'}
```{python}
#| echo: false
#| label: French-perf

country_recall('fr')
```
::: 

::: {.card title='Dutch (recall)'}
```{python}
#| echo: false
#| label: Dutch-perf

country_recall('nl', col_wrap=None)
```
::: 

::: {.card title='Portuguese (recall)'}
```{python}
#| echo: false
#| label: Portuguese-perf

country_recall('pt', col_wrap=3)
```
::: 

::: {.card title='Russian (recall)'}
```{python}
#| echo: false
#| label: Russian-perf

country_recall('ru', col_wrap=3)
```
::: 

::: {.card title='Turkish (recall)'}
```{python}
#| echo: false
#| label: Turkish-perf

country_recall('tr', col_wrap=None)
```
::: 

::: {.card title='Chinese (recall)'}
```{python}
#| echo: false
#| label: Chinese-perf

country_recall('zh', col_wrap=None)
```
::: 