--- 
title: "Dialect Identification (dialectid)"
format: 
  dashboard:
    logo: images/ingeotec.png
    orientation: columns
    nav-buttons: [github]
    theme: cosmo
    scrolling: true    
execute:
  freeze: auto    
---

```{python} 
#| echo: false 
#| include: false
#| label: setup

from collections import defaultdict, Counter
from os.path import join, basename, isfile
import json
from glob import glob
import country_converter as coco
import pandas as pd
import numpy as np
from IPython.display import Markdown
import seaborn as sns
from CompStats import metrics
from CompStats import measurements
import cvxpy as cp
from sklearn.metrics import recall_score
from encexp.download import download
from encexp.utils import DialectID_URL
from microtc.utils import tweet_iterator
from microtc.utils import save_model, load_model
from dialectid import DialectId
sns.set_style('whitegrid')
sns.set_theme(# font_scale=1.2,
              context='paper')
ORDER = ['DialectId[19]', 'DialectId[18]', 'DialectId[17]',
         'DialectId[19] (prob)', 'DialectId[18] (prob)',
         'DialectId[17] (prob)',
         'DialectId[19] (262k)', 'DialectId[18] (262k)',
         'DialectId[17] (262k)', 'StackBoW (262k)',
         'DialectId[19] (Orig. Dist.)']
INFO_ES = {
    'mx': ['Mexico', 'World'],
    'cl': ['Chile', 'South'],
    'es': ['Spain', 'World'],
    'ar': ['Argentina', 'South'],
    'co': ['Colombia', 'South'],
    'pe': ['Peru', 'South'],
    've': ['Venezuela', 'South'],
    'do': ['Dominican Republic', 'Caribbean'],
    'py': ['Paraguay', 'South'],
    'ec': ['Ecuador', 'South'],
    'uy': ['Uruguay', 'South'],
    'cr': ['Costa Rica', 'Central'],
    'sv': ['El Salvador', 'Central'],
    'pa': ['Panama', 'Central'],
    'gt': ['Guatemala', 'Central'],
    'hn': ['Honduras', 'Central'],
    'ni': ['Nicaragua', 'Central'],
    'bo': ['Bolivia', 'South'],
    'cu': ['Cuba', 'Caribbean'],
    'gq': ['Equatorial Guinea', 'World'],
    'pr': ['Puerto Rico', 'Caribbean']}


def dataset_info(lang='es'):
    dataset = download('dialectid_dataset_info',
                       base_url=DialectID_URL)
    dataset = {data['set']: {k: v for k, v in data.items() 
                             if k not in ('lang', 'set')}
               for data in dataset if data['lang'] == lang}
    dataset = pd.DataFrame(dataset) # .reset_index(names='Country')
    dataset.sort_index(inplace=True)
    orig_dist = download('dialectid_orig_dist_info',
                         base_url=DialectID_URL)
    orig_dist = {data['set']: {k: v for k, v in data.items() 
                             if k not in ('lang', 'set')}
                 for data in orig_dist if data['lang'] == lang}
    orig_dist = pd.DataFrame(orig_dist)  # .reset_index(names='Country')
    orig_dist.sort_index(inplace=True)
    dataset['train (orig. dist.)'] = orig_dist['train']
    dataset['test (orig. dist.)'] = orig_dist['test']
    # dataset.reset_index(names='Country', inplace=True)
    df = dataset.sort_values('train (orig. dist.)', ascending=False)
    df_all = pd.read_json(download(f'{lang}_info', return_path=True))
    df['Corpus'] = (df_all.sum(axis=0))[df.index] 
    df = pd.concat((df, pd.DataFrame([df.sum(axis=0).values],
                    columns=df.columns, index=['Sum'])), axis=0)
    df.reset_index(inplace=True, names=['Country'])
    xxx = coco.convert(df.Country[:-1], to='name_short')
    xxx.append(df.Country.iloc[-1])
    df.Country = xxx
    return df


def performance(lang, score, prefix='',
                value_name='macro-recall', **kwargs):
    fname = f'perf/{prefix}{lang}.gz'
    if isfile(fname):
        return load_model(fname)
    pred_dirname = f'dialectid-datasets/predictions/{lang}'
    for alg in ORDER:
        fname_pred = join(pred_dirname, f'{alg}.json')
        data = next(tweet_iterator(fname_pred))
        key = basename(fname_pred).split('.json')[0]
        if key == 'y':
            continue
        score(np.array(data), name=key)

    _ = score.dataframe(comparison=True,
                        value_name=value_name, **kwargs)
    _['Language'] = lang
    save_model(_, fname)
    return _


def country_recall(lang, col_wrap=5):
    detect = DialectId(lang=lang)
    pred_dirname = f'dialectid-datasets/predictions/{lang}'
    gold = next(tweet_iterator(join(pred_dirname, 'y.json')))
    score = metrics.recall_score(np.array(gold['y']), average=None)

    df = performance(lang, score, prefix='recall_', value_name='Recall',
                     var_name='Country',
                     perf_names=[f'{cntr}' for cntr in detect.countries])
    df.drop(columns=['Language'], inplace=True)
    ci = lambda x: measurements.CI(x, alpha=0.05)
    f_grid = sns.catplot(df, x='Recall', y='Algorithm', col_wrap=col_wrap,
                         capsize=0.2, linestyle='none', col='Country',
                         order=ORDER,
                         kind='point', errorbar=ci, # sharex=False,
                         hue='Comparison')
    return f_grid


def prior_proba(lang):
    freq = download('freq_countries_lang')
    data = freq[lang]['counter']
    del data['ALL']
    norm = sum(data.values())
    return {k: v / norm for k, v in data.items()}


def hprob_test(lang):
    fname = f'dialectid-datasets/predictions/dist-{lang}/DialectId[19].json.gz'
    _ = next(tweet_iterator(fname))
    data = Counter()
    data.update(_)
    norm = sum(data.values())
    return  {k: v / norm for k, v in data.items()}


def orig_dist_hprob_test(lang):
    fname = f'dialectid-datasets/predictions/dist-{lang}/DialectId[19] (Orig. Dist.).json.gz'
    if not isfile(fname):
        return None
    _ = next(tweet_iterator(fname))
    data = Counter()
    data.update(_)
    norm = sum(data.values())
    return  {k: v / norm for k, v in data.items()}  


def prior_test(lang):
    fname = f'dialectid-datasets/predictions/dist-{lang}/y.json.gz'
    _ = next(tweet_iterator(fname))
    data = Counter()
    data.update(_['y'])
    norm = sum(data.values())
    return  {k: v / norm for k, v in data.items()}


def dist_all(lang):
    if not isfile(f'pred-all/{lang}.json.gz'):
        return None
    data = Counter()
    data.update(next(tweet_iterator(f'pred-all/{lang}.json.gz')))
    norm = sum(data.values())
    return  {k: v / norm for k, v in data.items()}


def dist_all_origin_dist(lang):
    if not isfile(f'pred-all/{lang}.json.gz'):
        return None
    data = Counter()
    data.update(next(tweet_iterator(f'pred-all/{lang}-orig-dist.json.gz')))
    norm = sum(data.values())
    return  {k: v / norm for k, v in data.items()}


def dist_lang(lang):
    """Distribution"""

    test_values = hprob_test(lang)
    prior_train = sorted([(k, v) for k, v in prior_test(lang).items()
                          if k in test_values], key=lambda x: x[1])
    df = pd.DataFrame(prior_train, columns=['Country', 'Prob.'])
    df['Dataset'] = 'Test set'
    df['Target'] = 'Measured'
    df = df.sort_values(by='Prob.', ascending=False)[:21]
    countries = set(df.Country)
    for func, name, origin in zip([hprob_test,
                                   orig_dist_hprob_test, dist_all,
                                   dist_all_origin_dist],
                                  ['Test set', 'Test set',
                                   'w/o Geo. Inf.',
                                   'w/o Geo. Inf.'],
                                  ['DialectId',
                                   'DialectId (Orig. Dist.)',
                                   'DialectId', 'DialectId (Orig. Dist.)']):
        _info = func(lang)
        if _info is None:
            continue
        df2 = pd.DataFrame([(cntr, _info.get(cntr, 0), name)
                            for cntr, _ in prior_train],
                            columns=['Country', 'Prob.', 'Dataset'])
        mask = [x in countries for x in df2.Country]
        df2 = df2.loc[mask]
        df2['Target'] = origin
        df = pd.concat((df, df2))
    df.Country = coco.convert(df.Country, to='name_short')
    ax = sns.lineplot(df, x='Country', y='Prob.',
                      style='Target', hue='Target', size='Dataset')
    ax.tick_params(axis='x', rotation=90)
    return ax


def country_dist(country, lang='es', drop=None):
    """Country distribution of language"""
    # info_dialect = {v[0]: v for k, v in INFO_ES.items()}
    color = [x for x in sns.color_palette('Set1')][:5]
    dashes = []
    hue_order = None
    dataframe = pd.DataFrame()
    inferior = None
    superior = None
    for alg in [0, 1]:
        df = pd.read_json(f'countries/{lang}_{country}_{alg}.json.gz')
        if drop is not None:
            df.drop(columns=drop, inplace=True)
        if lang == 'es':
            df.columns = [INFO_ES[k][0] for k in df.columns]
        if alg == 0:
            hue_order = df.sum(axis=0).sort_values(ascending=False).index.tolist()
            if len(hue_order) >= 19:
                superior = hue_order[:19]
                inferior = hue_order[19:]
                df['Rest'] = df[inferior].sum(axis=1)
                hue_order = hue_order[:20]
                hue_order[-1] = 'Rest'
            for dash in ['', (1, 10), (5, 5), (1, 1)]:
                dashes.extend([dash] * len(color))
            dashes = (dashes * int(1 + len(hue_order) / len(dashes)))[:len(hue_order)]
            color = (color * int(1 + len(hue_order) / len(color)))[:len(hue_order)]
        if inferior is not None:
            _ = set(inferior).intersection(df.columns)
            df.drop(columns=_, inplace=True)
        df2 = df.rolling(window=7 * 12).sum()
        df2.dropna(inplace=True)
        df2 = df2.divide(df2.sum(axis=1), axis=0)
        df2 = df2.melt(ignore_index=False, value_name='Probability', var_name='Dialect')
        df2.reset_index(inplace=True, names='Date')
        df2['Algorithm'] = 'DialectId' if alg == 0 else 'DialectId (Orig. Dist.)'
        dataframe = pd.concat((dataframe, df2))
    _ = sns.relplot(dataframe, kind='line', x='Date', col='Algorithm', col_wrap=2,
                    hue_order=hue_order, style_order=hue_order,
                    col_order=['DialectId', 'DialectId (Orig. Dist.)'],
                    palette=color,
                    dashes=dashes, y='Probability',
                    style='Dialect', hue='Dialect')
    return _
    # order = pd.DataFrame()
    # for key in hue_order:
    #     order = pd.concat((order, dataframe.loc[dataframe.Dialect == key]))    
    # return px.line(order, x='Date', y='Probability',
    #                color='Dialect', facet_col='Algorithm')


def distribution_by_time(lang):
    """Original distribution by time"""
    df = pd.read_json(download(f'{lang}_info', return_path=True))
    df.drop(columns=['ALL'], inplace=True)
    color = [x for x in sns.color_palette('Set1')][:5]
    todos = df.sum(axis=0).sort_values(ascending=False).index.tolist()
    superior = todos[:19]
    columns_desc = coco.convert(superior, to='name_short')
    inferior = todos[19:]
    if len(inferior):
        df['Rest'] = df[inferior].sum(axis=1)
        superior.append('Rest')
        columns_desc.append('Rest')
    hue_order = columns_desc
    df = df[superior]
    df.columns = columns_desc
    dashes = []    
    for dash in ['', (1, 10), (5, 5), (1, 1)]:
        dashes.extend([dash] * len(color))
    dashes = (dashes * int(1 + len(hue_order) / len(dashes)))[:len(hue_order)]
    color = (color * int(1 + len(hue_order) / len(color)))[:len(hue_order)]
    df2 = df.rolling(window=7).mean()
    df2.dropna(inplace=True)
    # df2 = df2.divide(df2.sum(axis=1), axis=0)
    df2 = df2.melt(ignore_index=False, value_name='Number of tweets',
                   var_name='Country')
    df2.reset_index(inplace=True, names='Date')
    fig = sns.relplot(df2, kind='line', x='Date',
                      hue_order=hue_order, style_order=hue_order,
                      palette=color,
                      dashes=dashes, y='Number of tweets',
                      style='Country', hue='Country')
    return fig
    # return sns.move_legend(fig, "upper right", ncol=3, frameon=True)


def hypothesis(lang, alg=0,
               countries=None):
    """Hypothesis"""
    columns = None
    data = []
    for country in countries:
        fname = f'countries/{lang}_{country}_{alg}.json.gz'
        df2 = pd.read_json(fname)
        df2 = df2.rolling(window=7 * 12).sum()
        df2.dropna(inplace=True)
        df2 = df2.divide(df2.sum(axis=1), axis=0)
        if columns is None:
            columns = sorted(df2.columns)
        data.append(df2)

    index = data[0].index
    for d in data[1:]:
        index = index.intersection(d.index)

    P = cp.Parameter((len(columns), len(countries)))
    T = cp.Variable((len(countries), len(columns)))
    # obj = cp.Maximize(cp.min(cp.diag(T @ P)))
    obj = cp.Maximize(cp.sum(cp.diag(T @ P)))
    constraints = [T.sum(axis=0) == 1, T >= 0]
    prob = cp.Problem(obj, constraints)

    sol = []
    for value in index:
        P.value = np.array([d.loc[value][columns].values for d in data]).T
        prob.solve()
        sol.append(T.value)
    sol = np.array(sol)

    hipo = []
    for idx in range(len(columns)):
        cnt = Counter()
        cnt.update(countries[sol[:, :, idx].argmax(axis=1)].tolist())
        hipo.append(dict(cnt))    
    columns_desc = coco.convert(columns, to='name_short')
    hipo = pd.DataFrame(hipo, index=columns_desc)
    hipo.fillna(0, inplace=True)
    return hipo.divide(hipo.sum(axis=1), axis=0)
``` 

# Introduction

## Column 

::: {.card title='Introduction' .flow}  
`dialectid` aims to develop a set of algorithms to detect the dialect of a given text. For example, given a text written in Spanish, `dialectid` predicts the Spanish-speaking country where the text comes from. 

`dialectid` is available for Arabic (ar), German (de), English (en), Spanish (es), French (fr), Dutch (nl), Portuguese (pt), Russian (ru), Turkish (tr), and Chinese (zh).
:::

::: {.card title='Installing using conda' .flow}

`dialectid` can be install using the conda package manager with the following instruction.

```{sh} 
conda install --channel conda-forge dialectid
``` 
::: 

::: {.card title='Installing using pip' .flow} 
A more general approach to installing `dialectid` is through the use of the command pip, as illustrated in the following instruction.

```{sh} 
pip install dialectid
```
::: 

## Column

::: {.card title='Countries' .flow}
```{python}
#| echo: true
#| label: countries

from dialectid import DialectId
detect = DialectId(lang='es')
detect.countries
```
:::


# Quickstart

## Column 

::: {.card title='Dialect Identification' .flow}
```{python} 
#| echo: true
#| label: Identification

from dialectid import DialectId
detect = DialectId(lang='es')
detect.predict(['comiendo unos tacos',
                'acompañando el asado con un buen vino'])
```
:::

::: {.card title='Decision Function' .flow}
```{python} 
#| echo: true
#| label: Distance

from dialectid import DialectId
detect = DialectId(lang='es')
df = detect.decision_function(['acompañando el asado con un buen vino'])[0]
index = df.argsort()[::-1]
[(detect.countries[i], df[i]) for i in index
 if df[i] > 0]
```
:::

## Column 

::: {.card title='Probability' .flow}
```{python} 
#| echo: true
#| label: Probability

from dialectid import DialectId
detect = DialectId(lang='es', probability=True)
prob = detect.predict_proba(['acompañando el asado con un buen vino'])[0]
index = prob.argsort()[::-1]
[(detect.countries[i], prob[i])
 for i in index[:4]]
```
:::

# Corpora 

## Column {.tabset} 

::: {.card title='Arabic (ar)'}
```{python}
#| echo: false
#| label: fig-arabic
#| fig-cap: Number of tweets in the collection for the Arabic-speaking countries. 

distribution_by_time('ar')
```

```{python}
#| echo: false
#| label: tbl-arabic
#| tbl-cap: Number of tweets in the training and test sets for the Arabic-speaking countries. 

Markdown(dataset_info('ar').to_markdown(index=False))
```
:::

::: {.card title='German (de)'}
```{python}
#| echo: false
#| label: fig-german
#| fig-cap: Number of tweets in the collection for the German-speaking countries. 

distribution_by_time('de')
```

```{python}
#| echo: false
#| label: tbl-german
#| tbl-cap: Number of tweets in the training and test sets for the German-speaking countries.

Markdown(dataset_info('de').to_markdown(index=False))
```
:::

::: {.card title='English (en)'}
```{python}
#| echo: false
#| label: fig-english
#| fig-cap: Number of tweets in the collection for the English-speaking countries. 

distribution_by_time('en')
```

```{python}
#| echo: false
#| label: tbl-english
#| tbl-cap: Number of tweets in the training and test sets for the English-speaking countries.

Markdown(dataset_info('en').to_markdown(index=False))
```
:::

::: {.card title='Spanish (es)'}
```{python}
#| echo: false
#| label: fig-spanish
#| fig-cap: Number of tweets in the collection for the Spanish-speaking countries. 

distribution_by_time('es')
```

```{python}
#| echo: false
#| label: tbl-spanish
#| tbl-cap: Number of tweets in the training and test sets for the Spanish-speaking countries.

Markdown(dataset_info('es').to_markdown(index=False))
```
:::

::: {.card title='French (fr)'}
```{python}
#| echo: false
#| label: fig-french
#| fig-cap: Number of tweets in the collection for the French-speaking countries. 

distribution_by_time('fr')
```

```{python}
#| echo: false
#| label: tbl-french
#| tbl-cap: Number of tweets in the training and test sets for the French-speaking countries.

Markdown(dataset_info('fr').to_markdown(index=False))
```
:::

::: {.card title='Dutch (nl)'}
```{python}
#| echo: false
#| label: fig-dutch
#| fig-cap: Number of tweets in the collection for the Dutch-speaking countries. 

distribution_by_time('nl')
```

```{python}
#| echo: false
#| label: tbl-dutch
#| tbl-cap: Number of tweets in the training and test sets for the Dutch-speaking countries.

Markdown(dataset_info('nl').to_markdown(index=False))
```
:::

::: {.card title='Portuguese (pt)'}
```{python}
#| echo: false
#| label: fig-protuguese
#| fig-cap: Number of tweets in the collection for the Portuguese-speaking countries. 

distribution_by_time('pt')
```

```{python}
#| echo: false
#| label: tbl-portuguese
#| tbl-cap: Number of tweets in the training and test sets for the Portuguese-speaking countries.

Markdown(dataset_info('pt').to_markdown(index=False))
```
:::

::: {.card title='Russian (ru)'}
```{python}
#| echo: false
#| label: fig-russian
#| fig-cap: Number of tweets in the collection for the Russian-speaking countries. 

distribution_by_time('ru')
```

```{python}
#| echo: false
#| label: tbl-russian
#| tbl-cap: Number of tweets in the training and test sets for the Russian-speaking countries.

Markdown(dataset_info('ru').to_markdown(index=False))
```
:::

::: {.card title='Turkish (tr)'}
```{python}
#| echo: false
#| label: fig-turkish
#| fig-cap: Number of tweets in the collection for the Turkish-speaking countries. 

distribution_by_time('tr')
```

```{python}
#| echo: false
#| label: tbl-turkish
#| tbl-cap: Number of tweets in the training and test sets for the Turkish-speaking countries.

Markdown(dataset_info('tr').to_markdown(index=False))
```
:::

::: {.card title='Chinese (zh)'}
```{python}
#| echo: false
#| label: fig-chinese
#| fig-cap: Number of tweets in the collection for the Chinese-speaking countries. 

distribution_by_time('zh')
```

```{python}
#| echo: false
#| label: tbl-chinese
#| tbl-cap: Number of tweets in the training and test sets for the Chinese-speaking countries.

Markdown(dataset_info('zh').to_markdown(index=False))
```
:::

## Column 

::: {.card title="Description"}
The dataset used to create the self-supervised problems is a collection of Tweets collected from the open stream for several years, i.e., the Spanish collection started on December 11, 2015; English on July 1, 2016; Arabic on January 25, 2017; Russian on October 16, 2018; and the rest of the languages on June 1, 2021. In all the cases, the last day collected was June 9, 2023. The collected Tweets were filtered with the following restrictions: retweets were removed; URLs and usernames were replaced by the tokens _url and _usr, respectively; and only tweets with at least 50 characters were included in the final collection. 

The corpora are used to create two pairs of training and test sets. The training sets are drawn from tweets published before October 1, 2022, and the test sets are taken from tweets published on or after October 3, 2022. The procedure for creating the set pairs consists of two stages. In the first stage, the tweets were organized by country and then selected to form a uniform distribution by day. Within each day, near duplicates were removed. Then, a three-day sliding window was used to remove near duplicates within the window. The final step was to shuffle the data to remove the ordering by date, respecting the limit between the training and test sets.

The tweets of the first pair were selected to follow a uniform distribution by country as closely as possible. In this pair, the size of the training set is roughly 2 million tweets, whereas the test set size is $2^{12}$ (4,096) tweets per country. We also produce a smaller training set containing 262 thousand tweets. The procedure is equivalent to the previous one, aiming to have a uniform distribution of the countries. 

It is worth mentioning that we did not have enough information for all the countries and languages to follow an exactly uniform distribution. For example, @tbl-spanish (Spanish) notes that for Puerto Rico (pr), there are only 12,407 tweets in the training set and 1,487 tweets in the test set, corresponding to the total number of available tweets that meet the imposed restrictions. 

The second pair of tweets was selected to follow the original distribution of the corpus; in this case, the training and test set has a maximum size of 2 million tweets. The process of selecting the tweets was set as a convex optimization problem where the objective is to maximize the number of tweets subject to a maximum of 2 million ($2^{21}$), and the availability of tweets for each country, and the distribution is given by all the tweets available. In the tables, it can be observed that for Arabic, English, Spanish, French, Portuguese, and Russian, the maximum number of tweets is almost achieved in the training and test sets. 
:::

# Performance

## Column {.tabset}

::: {.card title='Macro-recall'}
```{python}
#| echo: false
#| tbl-cap: Performance of the different algorithms and languages.
#| label: tbl-macro-recall

if not isfile('perf/uniform_dist.json'):    
    df = pd.DataFrame()
    todos = []
    index = []
    for lang in ['es', 'en', 'ar',
                'de', 'fr', 'nl',
                'pt', 'ru', 'tr',
                'zh']:
        pred_dirname = f'dialectid-datasets/predictions/{lang}'
        gold = np.array(next(tweet_iterator(join(pred_dirname, 'y.json')))['y'])
        row = {}
        for alg in ORDER:
            fname_pred = join(pred_dirname, f'{alg}.json')
            data = next(tweet_iterator(fname_pred))
            key = basename(fname_pred).split('.json')[0]
            if key == 'y':
                continue
            row[key] = recall_score(gold, np.array(data), average='macro')
        todos.append(row)
        index.append(lang)
    df = pd.DataFrame(todos, index=['Spanish', 'English', 'Arabic',
                                    'German', 'French', 'Dutch',
                                    'Portuguese', 'Russian', 'Turkish', 'Chinese'])
    df.to_json('perf/uniform_dist.json')
else:
    df = pd.read_json('perf/uniform_dist.json')
Markdown(df.T.reset_index(names='Language').to_markdown(index=False, floatfmt=".4f"))
```

```{python}
#| echo: false
#| fig-cap: Performance of the different algorithms and languages. 
#| label: fig-macro-recall

df = pd.DataFrame()
for lang in ['es', 'en', 'ar',
             'de', 'fr', 'nl',
             'pt', 'ru', 'tr',
             'zh']:
    pred_dirname = f'dialectid-datasets/predictions/{lang}'
    gold = next(tweet_iterator(join(pred_dirname, 'y.json')))
    if 'score' in gold:
        score = gold['score']
    else:
        score = 'macro_recall'
    score = getattr(metrics, score)(np.array(gold['y']))
    _ = performance(lang, score)
    df = pd.concat((df, _))
ci = lambda x: measurements.CI(x, alpha=0.05)
f_grid = sns.catplot(df, x='macro-recall', y='Algorithm', col_wrap=3,
                     capsize=0.2, linestyle='none', col='Language',
                     order=ORDER,
                     kind='point', errorbar=ci, sharex=False,
                     hue='Comparison')
```
:::


::: {.card title='Macro-recall (Orig. Dist.)'}
```{python}
#| echo: false
#| tbl-cap: Performance of the different algorithms and languages on the original distribution.
#| label: tbl-macro-recall-orig-dist

if not isfile('perf/orig_dist.json'):    
    df = pd.DataFrame()
    order = [x for x in ORDER if '262k' not in x]
    todos = []
    index = []
    for lang in ['es', 'en', 'ar',
                'de', 'fr', 'nl',
                'pt', 'ru', 'tr',
                'zh']:
        pred_dirname = f'dialectid-datasets/predictions/dist-{lang}'
        gold = np.array(next(tweet_iterator(join(pred_dirname, 'y.json.gz')))['y'])
        row = {}
        for alg in order:
            fname_pred = join(pred_dirname, f'{alg}.json.gz')
            data = next(tweet_iterator(fname_pred))
            key = basename(fname_pred).split('.json')[0]
            if key == 'y':
                continue
            row[key] = recall_score(gold, np.array(data), average='macro')
        todos.append(row)
        index.append(lang)
    df = pd.DataFrame(todos, index=['Spanish', 'English', 'Arabic',
                                    'German', 'French', 'Dutch',
                                    'Portuguese', 'Russian', 'Turkish', 'Chinese'])
    df.to_json('perf/orig_dist.json')
else:
    df = pd.read_json('perf/orig_dist.json')
Markdown(df.T.reset_index(names='Language').to_markdown(index=False, floatfmt=".4f"))
```
:::

::: {.card title='Arabic'}
```{python}
#| echo: false
#| label: fig-Arabic-dist
#| fig-cap: Distributions of Arabic-speaking countries.

dist_lang('ar')
```
:::

::: {.card title='German'}
```{python}
#| echo: false
#| label: fig-German-dist
#| fig-cap: Distributions of German-speaking countries. 

dist_lang('de')
```
:::

::: {.card title='English'}
```{python}
#| echo: false
#| label: fig-English-dist
#| fig-cap: Distributions of English-speaking countries.

dist_lang('en')
```
:::

::: {.card title='Spanish'}
```{python}
#| echo: false
#| label: fig-Spanish-dist
#| fig-cap: Distributions of Spanish-speaking countries.

dist_lang('es')
```
:::

::: {.card title='French'}
```{python}
#| echo: false
#| label: fig-French-dist
#| fig-cap: Distributions of French-speaking countries.

dist_lang('fr')
```
:::

::: {.card title='Dutch'}
```{python}
#| echo: false
#| label: fig-Dutch-dist
#| fig-cap: Distributions of Dutch-speaking countries.

dist_lang('nl')
```
:::

::: {.card title='Portuguese'}
```{python}
#| echo: false
#| label: fig-Portuguese-dist
#| fig-cap: Distributions of Portuguese-speaking countries.

dist_lang('pt')
```
:::

::: {.card title='Russian'}
```{python}
#| echo: false
#| label: fig-Russian-dist
#| fig-cap: Distributions of Russian-speaking countries.

dist_lang('ru')
```
:::

::: {.card title='Turkish'}
```{python}
#| echo: false
#| label: fig-Turkish-dist
#| fig-cap: Distributions of Turkish-speaking countries.

dist_lang('tr')
```
:::

::: {.card title='Chinese'}
```{python}
#| echo: false
#| label: fig-Chinese-dist
#| fig-cap: Distributions of Chinese-speaking countries.

dist_lang('zh')
```
:::

## Column 

::: {.card title='Performance'}

The performance of different algorithms is presented in @fig-macro-recall using macro-recall. The best-performing system in almost all cases is DialectId, which is trained on 2 million tweets and has a vocabulary of 500,000 tokens. The exception are Turkish and Dutch, where the best systems is StackBoW trained with only 262k tweets.

The remaining figures provide details on macro-recall by presenting the system's recall in each country. 
::: 

# Spanish

## Column {.tabset .flow} 

::: {.card title='United States'}
```{python}
#| echo: false
#| fig-cap: Distribution estimated with DialectId on the Tweets having as geographic informacion United States. 
#| label: fig-es-us

country_dist('us')
```
:::

::: {.card title='Brazil'}
```{python}
#| echo: false
#| fig-cap: Distribution estimated with DialectId on the Tweets having as geographic informacion Brazil. 
#| label: fig-es-br

country_dist('br')
```
:::

::: {.card title='Great Britain'}
```{python}
#| echo: false
#| fig-cap: Distribution estimated with DialectId on the Tweets having as geographic informacion Great Britain. 
#| label: fig-es-gb

country_dist('gb')
```
:::

::: {.card title='Italy'}
```{python}
#| echo: false
#| fig-cap: Distribution estimated with DialectId on the Tweets having as geographic informacion Italy. 
#| label: fig-es-it

country_dist('it')
```
:::

::: {.card title='France'}
```{python}
#| echo: false
#| fig-cap: Distribution estimated with DialectId on the Tweets having as geographic informacion France. 
#| label: fig-es-fr

country_dist('fr')
```
:::

::: {.card title='Canada'}
```{python}
#| echo: false
#| fig-cap: Distribution estimated with DialectId on the Tweets having as geographic informacion Canada. 
#| label: fig-es-ca

country_dist('ca')
```
:::

::: {.card title='Germany'}
```{python}
#| echo: false
#| fig-cap: Distribution estimated with DialectId on the Tweets having as geographic informacion Germany. 
#| label: fig-es-de

country_dist('de')
```
:::

::: {.card title='Portugal'}
```{python}
#| echo: false
#| fig-cap: Distribution estimated with DialectId on the Tweets having as geographic informacion Portugal. 
#| label: fig-es-pt

country_dist('pt')
```
:::

## Column 

::: {.card title="Description"}

```{python}
#| echo: false
#| label: tbl-spanish-dest
#| tbl-cap: Probability of the origin of Tweets in different non-Spanish-speaking countries. 

countries = np.array(['us', 'br', 'gb', 'it', 'fr', 'ca', 'de', 'pt'])
df = hypothesis('es', countries=countries)
df = df[countries]
df.columns = coco.convert(df.columns, to='name_short')
Markdown(df.reset_index(names=['Country']).to_markdown(index=False, floatfmt=".3f"))
```
:::

# English

## Column {.tabset} 

::: {.card title='Malaysia'}
```{python}
#| echo: false
#| fig-cap: Distribution estimated with DialectId on the Tweets having as geographic informacion Malaysia. 
#| label: fig-en-my

country_dist('my', lang='en')
```
:::

::: {.card title='Indonesia'}
```{python}
#| echo: false
#| fig-cap: Distribution estimated with DialectId on the Tweets having as geographic informacion Indonesia. 
#| label: fig-en-id

country_dist('id', lang='en')
```
:::


::: {.card title='Brasil'}
```{python}
#| echo: false
#| fig-cap: Distribution estimated with DialectId on the Tweets having as geographic informacion Brasil. 
#| label: fig-en-br

country_dist('br', lang='en')
```
:::

::: {.card title='Germany'}
```{python}
#| echo: false
#| fig-cap: Distribution estimated with DialectId on the Tweets having as geographic informacion Germany. 
#| label: fig-en-de

country_dist('de', lang='en')
```
:::

::: {.card title='Spain'}
```{python}
#| echo: false
#| fig-cap: Distribution estimated with DialectId on the Tweets having as geographic informacion Spain. 
#| label: fig-en-es

country_dist('es', lang='en')
```
:::

::: {.card title='France'}
```{python}
#| echo: false
#| fig-cap: Distribution estimated with DialectId on the Tweets having as geographic informacion France. 
#| label: fig-en-fr

country_dist('fr', lang='en')
```
:::

::: {.card title='Italy'}
```{python}
#| echo: false
#| fig-cap: Distribution estimated with DialectId on the Tweets having as geographic informacion Italy. 
#| label: fig-en-it

country_dist('it', lang='en')
```
:::

::: {.card title='United Arab Emirates'}
```{python}
#| echo: false
#| fig-cap: Distribution estimated with DialectId on the Tweets having as geographic informacion United Arab Emirates. 
#| label: fig-en-ae

country_dist('ae', lang='en')
```
:::


## Column 

::: {.card title="Description"}

```{python}
#| echo: false
#| label: tbl-english-dest
#| tbl-cap: Probability of the origin of Tweets in different non-English-speaking countries. 

countries = np.array(['my', 'id', 'br', 'de', 'es', 'fr', 'it', 'ae'])
df = hypothesis('en', countries=countries)
df = df[countries]
df.columns = coco.convert(df.columns, to='name_short')
Markdown(df.reset_index(names=['Country']).to_markdown(index=False, floatfmt=".3f"))
```
:::


# Arabic

## Column {.tabset} 

::: {.card title='United States'}
```{python}
#| echo: false
#| fig-cap: Distribution estimated with DialectId on the Tweets having as geographic informacion United States. 
#| label: fig-ar-us

country_dist('us', lang='ar')
```
:::

::: {.card title='Great Britain'}
```{python}
#| echo: false
#| fig-cap: Distribution estimated with DialectId on the Tweets having as geographic informacion Great Britain. 
#| label: fig-ar-gb

country_dist('gb', lang='ar')
```
:::

::: {.card title='Turkey'}
```{python}
#| echo: false
#| fig-cap: Distribution estimated with DialectId on the Tweets having as geographic informacion Turkey. 
#| label: fig-ar-tr

country_dist('tr', lang='ar')
```
:::

::: {.card title='Germany'}
```{python}
#| echo: false
#| fig-cap: Distribution estimated with DialectId on the Tweets having as geographic informacion Germany. 
#| label: fig-ar-de

country_dist('de', lang='ar')
```
:::

::: {.card title='France'}
```{python}
#| echo: false
#| fig-cap: Distribution estimated with DialectId on the Tweets having as geographic informacion France. 
#| label: fig-ar-fr

country_dist('fr', lang='ar')
```
:::

::: {.card title='Canada'}
```{python}
#| echo: false
#| fig-cap: Distribution estimated with DialectId on the Tweets having as geographic informacion Canada. 
#| label: fig-ar-ca

country_dist('ca', lang='ar')
```
:::

::: {.card title='Australia'}
```{python}
#| echo: false
#| fig-cap: Distribution estimated with DialectId on the Tweets having as geographic informacion Australia. 
#| label: fig-ar-au

country_dist('au', lang='ar')
```
:::

::: {.card title='Italy'}
```{python}
#| echo: false
#| fig-cap: Distribution estimated with DialectId on the Tweets having as geographic informacion Italy. 
#| label: fig-ar-it

country_dist('it', lang='ar')
```
:::

## Column 

::: {.card title="Description"}

```{python}
#| echo: false
#| label: tbl-arabic-dest
#| tbl-cap: Probability of the origin of Tweets in different non-Arabic-speaking countries. 

countries = np.array(['us', 'gb', 'tr', 'de', 'fr', 'ca', 'au', 'it'])
df = hypothesis('ar', countries=countries)
df = df[countries]
df.columns = coco.convert(df.columns, to='name_short')
Markdown(df.reset_index(names=['Country']).to_markdown(index=False, floatfmt=".3f"))
```
:::


# French

## Column {.tabset} 

::: {.card title='United States'}
```{python}
#| echo: false
#| fig-cap: Distribution estimated with DialectId on the Tweets having as geographic informacion United States. 
#| label: fig-fr-us

country_dist('us', lang='fr')
```
:::

::: {.card title='Moroco'}
```{python}
#| echo: false
#| fig-cap: Distribution estimated with DialectId on the Tweets having as geographic informacion Moroco. 
#| label: fig-fr-ma

country_dist('ma', lang='fr')
```
:::

::: {.card title='Spain'}
```{python}
#| echo: false
#| fig-cap: Distribution estimated with DialectId on the Tweets having as geographic informacion Spain. 
#| label: fig-fr-es

country_dist('es', lang='fr')
```
:::

::: {.card title='Guadeloupe'}
```{python}
#| echo: false
#| fig-cap: Distribution estimated with DialectId on the Tweets having as geographic informacion Guadeloupe. 
#| label: fig-fr-gp

country_dist('gp', lang='fr')
```
:::

::: {.card title='Great Britain'}
```{python}
#| echo: false
#| fig-cap: Distribution estimated with DialectId on the Tweets having as geographic informacion Great Britain. 
#| label: fig-fr-gb

country_dist('gb', lang='fr')
```
:::

::: {.card title='Italy'}
```{python}
#| echo: false
#| fig-cap: Distribution estimated with DialectId on the Tweets having as geographic informacion Italy. 
#| label: fig-fr-it

country_dist('it', lang='fr')
```
:::

::: {.card title='Algeria'}
```{python}
#| echo: false
#| fig-cap: Distribution estimated with DialectId on the Tweets having as geographic informacion Algeria. 
#| label: fig-fr-dz

country_dist('dz', lang='fr')
```
:::

::: {.card title='Tanzania'}
```{python}
#| echo: false
#| fig-cap: Distribution estimated with DialectId on the Tweets having as geographic informacion Tanzania. 
#| label: fig-fr-tz

country_dist('tz', lang='fr')
```
:::

## Column 

::: {.card title="Description"}

```{python}
#| echo: false
#| label: tbl-french-dest
#| tbl-cap: Probability of the origin of Tweets in different non-French-speaking countries. 

countries = np.array(['us', 'ma', 'es', 'gp', 'gb', 'it', 'dz', 'tz'])
df = hypothesis('fr', countries=countries)
df = df[countries]
df.columns = coco.convert(df.columns, to='name_short')
Markdown(df.reset_index(names=['Country']).to_markdown(index=False, floatfmt=".3f"))
```
:::
