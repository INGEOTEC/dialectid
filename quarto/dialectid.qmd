--- 
title: "Dialect Identification (dialectid)"
format: 
  dashboard:
    logo: images/ingeotec.png
    orientation: columns
    nav-buttons: [github]
    theme: cosmo
execute:
  freeze: auto    
---

```{python} 
#| echo: false 
#| include: false
#| label: setup

from dialectid import BoW
from dialectid.utils import BASEURL, COUNTRIES, load_bow
from EvoMSA import BoW as eBoW
from EvoMSA.utils import Download
from microtc.utils import tweet_iterator
from os.path import isfile, join
import numpy as np
import json
from IPython.display import Markdown


def similarity(lang):
    cache = join('data', f'{lang}-similarity.json')
    if isfile(cache):
        return next(tweet_iterator(cache))
    _ = Download(f'{BASEURL}/stats-{lang}-train.json', f'stats-{lang}.json')
    data = next(tweet_iterator(f'stats-{lang}.json'))
    countries = [country for country in COUNTRIES[lang]
                 if data[country] > 2**22]
    bows = [BoW(lang=lang, loc=loc) for loc in countries]
    bows.append(BoW(lang=lang))
    tokens = [set(bow.names) for bow in bows]
    vocs = []
    for country in countries + [None]:
        freq = load_bow(lang=lang, d=17, loc=country)
        c = np.linalg.norm(list(freq.values()))
        vocs.append({k: v / c for k, v in freq.items()})
    countries.append('Uniform')        
    sim = np.ones((len(countries), len(countries)))
    for r in range(len(countries)):
        for c in range(r + 1, len(countries)):
            nom = len(tokens[r].intersection(tokens[c]))
            den = len(tokens[r].union(tokens[c]))
            sim[r, c] = nom / den
            sim[c, r] = sim[r, c]
    cos = np.ones((len(countries), len(countries)))
    for r in range(len(countries)):
        for c in range(r + 1, len(countries)):
            _ = sum([vocs[r][k] * vocs[c][k]
                     for k in tokens[r].intersection(tokens[c])])
            cos[r, c] = _
            cos[c, r] = _
    with open(cache, 'w') as fpt:
        print(json.dumps(dict(sim=sim.tolist(),
                              cos=cos.tolist(),
                              countries=countries)),
              file=fpt)
    return similarity(lang)
```

# Introduction

## Column 

::: {.card title='Introduction'}  
`dialectid` aims to develop a set of algorithms to detect the dialect of a given text. For example, given a text written in Spanish, `dialectid` predicts the Spanish-speaking country where the text comes from. 

`dialectid` is available for Arabic (ar), German (de), English (en), Spanish (es), French (fr), Dutch (nl), Portuguese (pt), Russian (ru), Turkish (tr), and Chinese (zh).
:::

::: {.card title='Installing using conda'}

`dialectid` can be install using the conda package manager with the following instruction.

```{sh} 
conda install --channel conda-forge dialectid
``` 
::: 

::: {.card title='Installing using pip'} 
A more general approach to installing `dialectid` is through the use of the command pip, as illustrated in the following instruction.

```{sh} 
pip install dialectid
```
::: 

## Column 

```{python}
#| echo: false
#| output: false

from dialectid import DialectId
detect = DialectId(lang='es', voc_size_exponent=15)
detect.countries
```

```{python} 
#| echo: true
#| title: Dialect Identification

from dialectid import DialectId
detect = DialectId(lang='es', voc_size_exponent=15)
detect.predict(['comiendo unos tacos',
                'tomando vino en la comida'])
```

```{python} 
#| echo: true
#| title: Decision Function

df = detect.decision_function('tomando vino en la comida')[0]
index = df.argsort()[::-1]
[(detect.countries[i], df[i]) for i in index
 if df[i] > 0]
```

# Corpus  

```{python} 
#| echo: false
#| output: false

from EvoMSA.utils import Download
from os.path import isfile
from microtc.utils import tweet_iterator
import pandas as pd
from dialectid.utils import COUNTRIES, BASEURL


def corpus_size(lang):
    data = []
    index = []
    for day, d in tweet_iterator(f'stats-{lang}.json.gz'):
        day = pd.to_datetime(day)
        data.append(d)
        index.append(day)
    df2 = pd.DataFrame(data, index=index)

    train = next(tweet_iterator(f'stats-{lang}-train.json'))
    test = next(tweet_iterator(f'stats-{lang}-test.json'))
    df = pd.DataFrame([train, test], index=['Train', 'Test'])
    df.columns.name = 'Countries'
    df.loc['Corpus'] = df2.sum(axis=0)
    columns = COUNTRIES[lang]
    df = df.reindex(['Corpus', 'Train', 'Test'])
    _ = df[columns].T.sort_values('Corpus', ascending=False)
    return Markdown(_.to_markdown())


def init_date(lang):
    date, data = next(tweet_iterator(f'stats-{lang}.json.gz'))
    return date


def end_date(lang):
    for date, data in tweet_iterator(f'stats-{lang}.json.gz'):
        pass
    return date


for lang in COUNTRIES:
    if isfile(f'stats-{lang}.json.gz'):
        continue
    Download(f'{BASEURL}/stats-{lang}-train.json', f'stats-{lang}-train.json')
    Download(f'{BASEURL}/stats-{lang}-test.json', f'stats-{lang}-test.json')
    Download(f'{BASEURL}/stats-{lang}.json.gz', f'stats-{lang}.json.gz')
```

## Column {.tabset} 

::: {.card title='Arabic (ar)'}

The table shows the dataset size for Arabic (ar) tweets collected from `{python} init_date('ar')` to `{python} end_date('ar')`. 

```{python}
#| echo: false
#| title: Data from `{python} init_date('ar')` to 

corpus_size('ar')
```

:::

::: {.card title='German (de)'}

The table shows the dataset size for German (de) tweets collected from `{python} init_date('de')` to `{python} end_date('de')`. 

```{python}
#| echo: false

corpus_size('de')
```
:::

::: {.card title='English (en)'}

The table shows the dataset size for English (en) tweets collected from `{python} init_date('en')` to `{python} end_date('en')`. 

```{python}
#| echo: false

corpus_size('en')
```
:::

::: {.card title='Spanish (es)'}

The table shows the dataset size for Spanish (es) tweets collected from `{python} init_date('es')` to `{python} end_date('es')`. 

```{python}
#| echo: false

corpus_size('es')
```
:::

::: {.card title='French (fr)'}

The table shows the dataset size for French (fr) tweets collected from `{python} init_date('fr')` to `{python} end_date('fr')`. 

```{python}
#| echo: false

corpus_size('fr')
```
:::

::: {.card title='Dutch (nl)'}

The table shows the dataset size for Dutch (nl) tweets collected from `{python} init_date('nl')` to `{python} end_date('nl')`. 

```{python}
#| echo: false

corpus_size('nl')
```
:::

::: {.card title='Portuguese (pt)'}

The table shows the dataset size for Portuguese (pt) tweets collected from `{python} init_date('pt')` to `{python} end_date('pt')`. 

```{python}
#| echo: false

corpus_size('pt')
```
:::

::: {.card title='Russian (ru)'}

The table shows the dataset size for Russian (ru) tweets collected from `{python} init_date('ru')` to `{python} end_date('ru')`. 

```{python}
#| echo: false

corpus_size('ru')
```
:::

::: {.card title='Turkish (tr)'}

The table shows the dataset size for Turkish (tr) tweets collected from `{python} init_date('tr')` to `{python} end_date('tr')`. 

```{python}
#| echo: false

corpus_size('tr')
```
:::

::: {.card title='Chinese (zh)'}

The table shows the dataset size for Chinese (zh) tweets collected from `{python} init_date('zh')` to `{python} end_date('zh')`. 

```{python}
#| echo: false

corpus_size('zh')
```
:::


## Column 

::: {.card title="Description"}
Tweets have been collected from the open stream for several years, e.g., the Spanish collection started on December 11, 2015 (see the table on the left to know the starting collection date for each language). The collected Tweets were filtered with the following restrictions: the retweets were removed; URL and users were replaced by the tokens _url and _usr, respectively; and only tweets with at least 50 characters were accepted in the final collection, namely Corpus. 

The Corpus is divided into two distinct sets: the first set is utilized to construct the training set, while the second set corresponds to the test set. The basis for this division is a specific date, with tweets published prior to October 1, 2022, forming the first set, and those published on October 3, 2022, or later, being used to create the test set. 

The training set and test set were created with an equivalent procedure; the only difference is that the maximum size of the training set is 10M tweets and $2^{12}$ (4096) tweets for the test set.

The training and test set was meticulously crafted by uniformly selecting the maximum number of tweets (i.e., 10M and $2^{12}$, respectively) from each day. These selected tweets were then organized by day, and within each day, the tweets were randomly chosen, with near duplicates being removed. The subsequent step involved the elimination of tweets that were near duplicates of the previous three days.

It is worth mentioning that the last step is to shuffle the training and test set to eliminate the ordering by date. 
:::


# Similarity

## Column {width="40%"}

### Tabset {.tabset height="50%"} 

```{python} 
#| echo: false 
#| title: Arabic (ar)

import plotly.express as px

data = similarity('ar')
countries = data['countries']
fig = px.imshow(data['sim'],
                labels=dict(x="Country", y="Country", color="Similarity"),
                x=countries,
                y=countries)
# fig.update_xaxes(side="top")
fig.show()
```

```{python} 
#| echo: false 
#| title: English (en)

import plotly.express as px

data = similarity('en')
countries = data['countries']
fig = px.imshow(data['sim'],
                labels=dict(x="Country", y="Country", color="Similarity"),
                x=countries,
                y=countries)
# fig.update_xaxes(side="top")
fig.show()
```

```{python} 
#| echo: false 
#| title: Spanish (es)

import plotly.express as px

data = similarity('es')
countries = data['countries']
fig = px.imshow(data['sim'],
                labels=dict(x="Country", y="Country", color="Similarity"),
                x=countries,
                y=countries)
# fig.update_xaxes(side="top")
fig.show()
```

### Tabset {.tabset height="50%"}

```{python} 
#| echo: false 
#| title: Arabic (ar)

import plotly.express as px

data = similarity('ar')
countries = data['countries']
fig = px.imshow(data['cos'],
                labels=dict(x="Country", y="Country", color="Similarity"),
                x=countries,
                y=countries)
# fig.update_xaxes(side="top")
fig.show()
```

```{python} 
#| echo: false 
#| title: English (en)

import plotly.express as px

data = similarity('en')
countries = data['countries']
fig = px.imshow(data['cos'],
                labels=dict(x="Country", y="Country", color="Similarity"),
                x=countries,
                y=countries)
# fig.update_xaxes(side="top")
fig.show()
```

```{python} 
#| echo: false 
#| title: Spanish (es)

import plotly.express as px

data = similarity('es')
countries = data['countries']
fig = px.imshow(data['cos'],
                labels=dict(x="Country", y="Country", color="Similarity"),
                x=countries,
                y=countries)
# fig.update_xaxes(side="top")
fig.show()
```


## Column {width="60%"} 

The figures on the left correspond to the Jaccard index (top) and Cosine similarity (bottom) between the vocabularies estimated from data coming from a particular country. It also includes the *Uniform* (e.g., `BoW(lang='es')`) vocabulary that corresponds to taking a uniform sample from all the regions.

For instance, the following code computes the similarity in Spanish between Mexico and Guatemala.

```{python}
#| echo: true
#| eval: false
#| label: jaccard

from dialectid import BoW

mx = BoW(lang='es', loc='mx')
gt = BoW(lang='es', loc='gt')
mx_voc = set(mx.names)
gt_voc = set(gt.names)
num = len(mx_voc.intersection(gt_voc))
den = len(mx_voc.union(gt_voc))
num / den
```

The following code exemplifies the Cosine similarity between Mexico and Guatemala.

```{python} 
#| echo: true
#| eval: false
#| label: cosine

import numpy as np
from dialectid.utils import load_bow

mx_freq = load_bow(lang='es', d=17, loc='mx')
_ = np.linalg.norm(list(mx_freq.values()))
mx_freq = {k: v / _ for k, v in mx_freq.items()}
gt_freq = load_bow(lang='es', d=17, loc='gt')
_ = np.linalg.norm(list(gt_freq.values()))
gt_freq = {k: v / _ for k, v in gt_freq.items()}
tokens = [token for token in mx_freq if token in gt_freq]
sum([mx_freq[token] * gt_freq[token]
     for token in tokens])
```


# Performance

## Column {.tabset}
```{python}
#| echo: false
#| title: Arabic (ar)
import pandas as pd
import plotly.express as px
df = pd.read_csv('data/ar-recall.csv', index_col=0)
df2 = df.sort_values(by=['Recall', 'System'])
fig = px.bar(df2.astype({'System': str}),
             x='Country', y='Recall',
             barmode='overlay', 
             color='System')
fig.show()
```

```{python}
#| echo: false
#| title: German (de)
import pandas as pd
df = pd.read_csv('data/de-recall.csv', index_col=0)
df2 = df.sort_values(by=['Recall', 'System'])
fig = px.bar(df2.astype({'System': str}),
             x='Country', y='Recall',
             barmode='overlay', 
             color='System')
fig.show()
```

```{python}
#| echo: false
#| title: English (en)
import pandas as pd
df = pd.read_csv('data/en-recall.csv', index_col=0)
df2 = df.sort_values(by=['Recall', 'System'])
fig = px.bar(df2.astype({'System': str}),
             x='Country', y='Recall',
             barmode='overlay', 
             color='System')
fig.show()
```

```{python}
#| echo: false
#| title: Spanish (es)
import pandas as pd
df = pd.read_csv('data/es-recall.csv', index_col=0)
df2 = df.sort_values(by=['Recall', 'System'])
fig = px.bar(df2.astype({'System': str}),
             x='Country', y='Recall',
             barmode='overlay', 
             color='System')
fig.show()
```

```{python}
#| echo: false
#| title: French (fr)
import pandas as pd
df = pd.read_csv('data/fr-recall.csv', index_col=0)
df2 = df.sort_values(by=['Recall', 'System'])
fig = px.bar(df2.astype({'System': str}),
             x='Country', y='Recall',
             barmode='overlay', 
             color='System')
fig.show()
```

```{python}
#| echo: false
#| title: Dutch (nl)
import pandas as pd
df = pd.read_csv('data/nl-recall.csv', index_col=0)
df2 = df.sort_values(by=['Recall', 'System'])
fig = px.bar(df2.astype({'System': str}),
             x='Country', y='Recall',
             barmode='overlay', 
             color='System')
fig.show()
```

```{python}
#| echo: false
#| title: Portuguese (pt)
import pandas as pd
df = pd.read_csv('data/pt-recall.csv', index_col=0)
df2 = df.sort_values(by=['Recall', 'System'])
fig = px.bar(df2.astype({'System': str}),
             x='Country', y='Recall',
             barmode='overlay', 
             color='System')
fig.show()
```

```{python}
#| echo: false
#| title: Russian (ru)
import pandas as pd
df = pd.read_csv('data/ru-recall.csv', index_col=0)
df2 = df.sort_values(by=['Recall', 'System'])
fig = px.bar(df2.astype({'System': str}),
             x='Country', y='Recall',
             barmode='overlay', 
             color='System')
fig.show()
```

```{python}
#| echo: false
#| title: Turkish (tr)
import pandas as pd
df = pd.read_csv('data/tr-recall.csv', index_col=0)
df2 = df.sort_values(by=['Recall', 'System'])
fig = px.bar(df2.astype({'System': str}),
             x='Country', y='Recall',
             barmode='overlay', 
             color='System')
fig.show()
```

```{python}
#| echo: false
#| title: Chinese (zh)
import pandas as pd
df = pd.read_csv('data/zh-recall.csv', index_col=0)
df2 = df.sort_values(by=['Recall', 'System'])
fig = px.bar(df2.astype({'System': str}),
             x='Country', y='Recall',
             barmode='overlay', 
             color='System')
fig.show()
```

## Column 

The figures on the left show the recall of the different countries, using four different vocabularies. [EvoMSA](http://evomsa.readthedocs.io) corresponds to the vocabulary estimated in our previous development; **Uniform** (e.g., `BoW(lang='es')`) is obtained by taking a uniform sample from all the regions; **q-grams** (e.g., `BoW(lang='es', loc='qgrams')`) which uses an uniform samples with the characteristic that the q-grams are only computed on the words;
and **Country** (e.g., `BoW(lang='es', loc='mx')`) is the vocabulary of a particular location. In all the cases, the vocabulary is estimated with  $2^{22}$ Tweets. Of course, there is not enough information for all the cases. Consequently, the vocabulary is not present for that configuration. 

The table below presents the average recall for the different languages and models. Since the Country model is not available for all countries, the missing values were filled with the corresponding Uniform's recall to compute the macro-recall for all the countries. 

```{python} 
#| echo: false
#| caption: Performance in terms of macro-recall
#| label: tab-macro-recall

from IPython.display import Markdown
import pandas as pd
perf = {}
for lang in ['ar', 'de', 'en',
             'es', 'fr', 'nl',
             'pt', 'ru', 'tr', 'zh']:
    df = pd.read_csv(f'data/{lang}-recall.csv', index_col=0)
    df.set_index(['Country', 'System'], inplace=True)
    df.sort_index(level='Country', inplace=True)
    df2 = df.unstack()
    df2.columns = df2.columns.get_level_values(1)
    if 'Country' in df2.columns:
        mask = df2.Country.isna()
        df2.loc[mask, 'Country'] = df2.Uniform.loc[mask]
    perf[lang] = df2.mean(axis=0)
_ = pd.DataFrame(perf).reindex(['EvoMSA', 'Uniform',
                                'q-grams','Country']).T
Markdown(_.to_markdown())
```